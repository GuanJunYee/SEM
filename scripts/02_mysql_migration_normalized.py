"""
02_mysql_migration_normalized.py
Creates a normalized MySQL database structure following SEM Demo patterns:
- Categories table
- Locations table  
- PaymentMethods table
- Customers table
- Items table (with CategoryID foreign key)
- Transactions table (with foreign keys to all reference tables)

Reads normalized CSV files from: dataset/normalized/ folder
(Generated by 01_data_pipeline_main.py)

Enhanced with SEM Demo features:
- Connection validation
- Batch processing
- Comprehensive error handling
- Data integrity checks
- Transaction management
- Idempotent writes (UPSERT operations)
- Per-batch reconciliation
- Enhanced audit logging
"""

import os
import pandas as pd
from pathlib import Path
from sqlalchemy import create_engine, text, MetaData, Table, Column, Integer, String, DECIMAL, Date, Boolean, ForeignKey
from sqlalchemy.types import VARCHAR, DECIMAL as SQLDecimal, Integer as SQLInteger, Date as SQLDate, Boolean as SQLBoolean
from sqlalchemy.exc import SQLAlchemyError
from dotenv import load_dotenv
import time
from datetime import datetime

# ---- Setup paths ----
ROOT = Path(__file__).resolve().parents[1]
DATA = ROOT / "dataset"
NORMALIZED_DATA = DATA / "normalized"  # New: Normalized CSV files folder
RESULTS = ROOT / "results"
RESULTS.mkdir(exist_ok=True)

# Input files (created by 01_data_pipeline_main.py in normalized folder)
CATEGORIES_CSV = NORMALIZED_DATA / "categories.csv"
LOCATIONS_CSV = NORMALIZED_DATA / "locations.csv"
PAYMENT_METHODS_CSV = NORMALIZED_DATA / "payment_methods.csv"
CUSTOMERS_CSV = NORMALIZED_DATA / "customers.csv"
ITEMS_CSV = NORMALIZED_DATA / "items.csv"
TRANSACTIONS_CSV = NORMALIZED_DATA / "transactions_normalized.csv"

LOG = RESULTS / "03_mysql_normalized_load_log.txt"
DDL = RESULTS / "03_mysql_normalized_schema.sql"

def log(msg: str):
    """Enhanced logging with timestamps (SEM Demo style)"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    formatted_msg = f"[{timestamp}] {msg}"
    print(formatted_msg)
    with open(LOG, "a", encoding="utf-8") as f:
        f.write(formatted_msg + "\n")

def validate_mysql_connection(engine):
    """Validate MySQL connection following SEM Demo patterns"""
    log(">> Validating MySQL connection...")
    
    try:
        with engine.connect() as connection:
            # Test basic connectivity
            result = connection.execute(text("SELECT 1"))
            result.fetchone()
            
            # Check MySQL version
            version_result = connection.execute(text("SELECT VERSION()"))
            mysql_version = version_result.fetchone()[0]
            log(f">> MySQL connection successful")
            log(f"   MySQL Version: {mysql_version}")
            
            # Check database existence
            db_check = connection.execute(text(f"SHOW DATABASES LIKE '{MYSQL_DB}'"))
            if db_check.fetchone():
                log(f">> Database '{MYSQL_DB}' exists")
            else:
                log(f"WARNING: Database '{MYSQL_DB}' does not exist - will be created")
            
            return True
            
    except SQLAlchemyError as e:
        log(f"ERROR: MySQL connection failed: {str(e)}")
        log("   Please check your database configuration in .env file")
        return False
    except Exception as e:
        log(f"ERROR: Unexpected error during connection validation: {str(e)}")
        return False

def check_input_files():
    """Validate that all required input files exist (SEM Demo pattern)"""
    log("ðŸ“ Checking input files...")
    
    required_files = {
        'Categories': CATEGORIES_CSV,
        'Locations': LOCATIONS_CSV,
        'Payment Methods': PAYMENT_METHODS_CSV,
        'Customers': CUSTOMERS_CSV,
        'Items': ITEMS_CSV,
        'Transactions': TRANSACTIONS_CSV
    }
    
    missing_files = []
    file_sizes = {}
    
    for name, file_path in required_files.items():
        if file_path.exists():
            size = file_path.stat().st_size
            file_sizes[name] = size
            log(f">> {name}: {file_path.name} ({size:,} bytes)")
        else:
            missing_files.append(name)
            log(f"ERROR: {name}: {file_path.name} (NOT FOUND)")
    
    if missing_files:
        log(f"ERROR: Missing required files: {', '.join(missing_files)}")
        log("   Please run 02_clean_dataset_normalized.py first")
        return False
    
    log(">> All required input files found")
    return True

# ---- Load environment config ----
load_dotenv(ROOT / ".env")
MYSQL_HOST = os.getenv("MYSQL_HOST", "localhost")
MYSQL_PORT = int(os.getenv("MYSQL_PORT", "3306"))
MYSQL_USER = os.getenv("MYSQL_USER", "root")
MYSQL_PASSWORD = os.getenv("MYSQL_PASSWORD", "")
MYSQL_DB = os.getenv("MYSQL_DB", "retail_db_normalized")

def mysql_url(db_name=None):
    """Create MySQL connection URL"""
    db = f"/{db_name}" if db_name else ""
    pwd = f":{MYSQL_PASSWORD}" if MYSQL_PASSWORD else ""
    return f"mysql+pymysql://{MYSQL_USER}{pwd}@{MYSQL_HOST}:{MYSQL_PORT}{db}?charset=utf8mb4"

def create_database():
    """Create the database if it doesn't exist"""
    server_engine = create_engine(mysql_url(), pool_pre_ping=True)
    
    with server_engine.connect() as conn:
        conn.execute(text(f"CREATE DATABASE IF NOT EXISTS `{MYSQL_DB}` CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;"))
        log(f"Ensured database exists: {MYSQL_DB}")

def create_normalized_schema(engine):
    """Create the normalized database schema with proper foreign keys"""
    
    log("\nCreating normalized database schema...")
    
    # DDL statements for creating tables
    ddl_statements = []
    
    # 1. Categories table
    categories_ddl = f"""
    CREATE TABLE IF NOT EXISTS `Categories` (
        `CategoryID` INT AUTO_INCREMENT PRIMARY KEY,
        `CategoryName` VARCHAR(64) NOT NULL UNIQUE,
        `CreatedAt` TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        `UpdatedAt` TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
    """
    ddl_statements.append(("Categories", categories_ddl))
    
    # 2. Locations table
    locations_ddl = f"""
    CREATE TABLE IF NOT EXISTS `Locations` (
        `LocationID` INT AUTO_INCREMENT PRIMARY KEY,
        `LocationName` VARCHAR(32) NOT NULL UNIQUE,
        `CreatedAt` TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        `UpdatedAt` TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
    """
    ddl_statements.append(("Locations", locations_ddl))
    
    # 3. PaymentMethods table
    payment_methods_ddl = f"""
    CREATE TABLE IF NOT EXISTS `PaymentMethods` (
        `PaymentMethodID` INT AUTO_INCREMENT PRIMARY KEY,
        `PaymentMethodName` VARCHAR(32) NOT NULL UNIQUE,
        `CreatedAt` TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        `UpdatedAt` TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
    """
    ddl_statements.append(("PaymentMethods", payment_methods_ddl))
    
    # 4. Customers table
    customers_ddl = f"""
    CREATE TABLE IF NOT EXISTS `Customers` (
        `CustomerID` VARCHAR(32) PRIMARY KEY,
        `CreatedAt` TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        `UpdatedAt` TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
    """
    ddl_statements.append(("Customers", customers_ddl))
    
    # 5. Items table (with foreign key to Categories)
    items_ddl = f"""
    CREATE TABLE IF NOT EXISTS `Items` (
        `ItemID` INT AUTO_INCREMENT PRIMARY KEY,
        `ItemName` VARCHAR(64) NOT NULL,
        `PricePerUnit` DECIMAL(10, 2) NOT NULL,
        `CategoryID` INT NOT NULL,
        `CreatedAt` TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        `UpdatedAt` TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
        FOREIGN KEY (`CategoryID`) REFERENCES `Categories`(`CategoryID`) ON DELETE RESTRICT ON UPDATE CASCADE,
        UNIQUE KEY `unique_item_category` (`ItemName`, `CategoryID`)
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
    """
    ddl_statements.append(("Items", items_ddl))
    
    # 6. Transactions table (with all foreign keys)
    transactions_ddl = f"""
    CREATE TABLE IF NOT EXISTS `Transactions` (
        `TransactionID` VARCHAR(32) PRIMARY KEY,
        `CustomerID` VARCHAR(32) NOT NULL,
        `ItemID` INT NOT NULL,
        `PaymentMethodID` INT NOT NULL,
        `LocationID` INT NOT NULL,
        `Quantity` INT NOT NULL,
        `TotalPrice` DECIMAL(10, 2) NOT NULL,
        `TransactionDate` DATE NOT NULL,
        `DiscountApplied` BOOLEAN DEFAULT FALSE,
        `CreatedAt` TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        `UpdatedAt` TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
        FOREIGN KEY (`CustomerID`) REFERENCES `Customers`(`CustomerID`) ON DELETE RESTRICT ON UPDATE CASCADE,
        FOREIGN KEY (`ItemID`) REFERENCES `Items`(`ItemID`) ON DELETE RESTRICT ON UPDATE CASCADE,
        FOREIGN KEY (`PaymentMethodID`) REFERENCES `PaymentMethods`(`PaymentMethodID`) ON DELETE RESTRICT ON UPDATE CASCADE,
        FOREIGN KEY (`LocationID`) REFERENCES `Locations`(`LocationID`) ON DELETE RESTRICT ON UPDATE CASCADE,
        INDEX `idx_customer` (`CustomerID`),
        INDEX `idx_item` (`ItemID`),
        INDEX `idx_date` (`TransactionDate`),
        INDEX `idx_payment_method` (`PaymentMethodID`),
        INDEX `idx_location` (`LocationID`)
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
    """
    ddl_statements.append(("Transactions", transactions_ddl))
    
    # Execute DDL statements
    with engine.begin() as conn:
        # Drop existing tables in reverse order (to handle foreign key constraints)
        drop_order = ["Transactions", "Items", "Customers", "PaymentMethods", "Locations", "Categories"]
        for table_name in drop_order:
            try:
                conn.execute(text(f"DROP TABLE IF EXISTS `{table_name}`;"))
                log(f"Dropped existing table: {table_name}")
            except Exception as e:
                log(f"Note: Could not drop {table_name}: {e}")
        
        # Create tables in correct order (respecting foreign key dependencies)
        for table_name, ddl in ddl_statements:
            try:
                conn.execute(text(ddl))
                log(f"Created table: {table_name}")
            except Exception as e:
                log(f"ERROR creating {table_name}: {e}")
                raise
    
    # Save DDL to file
    with open(DDL, "w", encoding="utf-8") as f:
        f.write("-- Normalized Database Schema for Retail Sales\n")
        f.write(f"-- Generated for database: {MYSQL_DB}\n\n")
        for table_name, ddl in ddl_statements:
            f.write(f"-- {table_name} table\n")
            f.write(ddl + "\n\n")
    
    log(f"Schema DDL saved to: {DDL}")

def load_reference_tables(engine):
    """Load data into reference tables first (no foreign key dependencies)"""
    
    log("\nLoading reference tables...")
    
    # 1. Load Categories (with idempotent writes)
    if CATEGORIES_CSV.exists():
        categories_df = pd.read_csv(CATEGORIES_CSV)
        log(f"Loading {len(categories_df)} categories with idempotent writes...")
        
        # Insert categories with UPSERT for idempotency
        with engine.begin() as conn:
            for _, row in categories_df.iterrows():
                conn.execute(text("""
                    INSERT INTO `Categories` (`CategoryName`) 
                    VALUES (:name)
                    ON DUPLICATE KEY UPDATE 
                        `CategoryName` = VALUES(`CategoryName`),
                        `UpdatedAt` = CURRENT_TIMESTAMP
                """), {"name": row['CategoryName']})
        
        # Verify count and reconcile
        with engine.connect() as conn:
            count = conn.execute(text("SELECT COUNT(*) FROM `Categories`")).scalar()
            log(f"Categories loaded: {count}")
            reconcile_batch_data(engine, "Categories", len(categories_df), " (categories migration)", is_final_check=True)
    
    # 2. Load Locations (with idempotent writes)
    if LOCATIONS_CSV.exists():
        locations_df = pd.read_csv(LOCATIONS_CSV)
        log(f"Loading {len(locations_df)} locations with idempotent writes...")
        
        with engine.begin() as conn:
            for _, row in locations_df.iterrows():
                conn.execute(text("""
                    INSERT INTO `Locations` (`LocationName`) 
                    VALUES (:name)
                    ON DUPLICATE KEY UPDATE 
                        `LocationName` = VALUES(`LocationName`),
                        `UpdatedAt` = CURRENT_TIMESTAMP
                """), {"name": row['LocationName']})
        
        with engine.connect() as conn:
            count = conn.execute(text("SELECT COUNT(*) FROM `Locations`")).scalar()
            log(f"Locations loaded: {count}")
            reconcile_batch_data(engine, "Locations", len(locations_df), " (locations migration)", is_final_check=True)
    
    # 3. Load Payment Methods (with idempotent writes)
    if PAYMENT_METHODS_CSV.exists():
        payment_methods_df = pd.read_csv(PAYMENT_METHODS_CSV)
        log(f"Loading {len(payment_methods_df)} payment methods with idempotent writes...")
        
        with engine.begin() as conn:
            for _, row in payment_methods_df.iterrows():
                conn.execute(text("""
                    INSERT INTO `PaymentMethods` (`PaymentMethodName`) 
                    VALUES (:name)
                    ON DUPLICATE KEY UPDATE 
                        `PaymentMethodName` = VALUES(`PaymentMethodName`),
                        `UpdatedAt` = CURRENT_TIMESTAMP
                """), {"name": row['PaymentMethodName']})
        
        with engine.connect() as conn:
            count = conn.execute(text("SELECT COUNT(*) FROM `PaymentMethods`")).scalar()
            log(f"Payment methods loaded: {count}")
            reconcile_batch_data(engine, "PaymentMethods", len(payment_methods_df), " (payment methods migration)", is_final_check=True)
    
    # 4. Load Customers (with idempotent writes)
    if CUSTOMERS_CSV.exists():
        customers_df = pd.read_csv(CUSTOMERS_CSV)
        log(f"Loading {len(customers_df)} customers with idempotent writes...")
        
        with engine.begin() as conn:
            for _, row in customers_df.iterrows():
                conn.execute(text("""
                    INSERT INTO `Customers` (`CustomerID`) 
                    VALUES (:id)
                    ON DUPLICATE KEY UPDATE 
                        `CustomerID` = VALUES(`CustomerID`),
                        `UpdatedAt` = CURRENT_TIMESTAMP
                """), {"id": row['CustomerID']})
        
        with engine.connect() as conn:
            count = conn.execute(text("SELECT COUNT(*) FROM `Customers`")).scalar()
            log(f"Customers loaded: {count}")
            reconcile_batch_data(engine, "Customers", len(customers_df), " (customers migration)", is_final_check=True)

def load_items_table(engine):
    """Load items table (depends on Categories) with idempotent writes"""
    
    log("\nLoading Items table with idempotent writes...")
    
    if not ITEMS_CSV.exists():
        log("ERROR: Items CSV file not found")
        return
    
    items_df = pd.read_csv(ITEMS_CSV)
    log(f"Loading {len(items_df)} items...")
    
    # Get category ID mappings
    with engine.connect() as conn:
        category_map = {}
        result = conn.execute(text("SELECT CategoryID, CategoryName FROM Categories"))
        for row in result:
            category_map[row[1]] = row[0]  # CategoryName -> CategoryID
    
    # Load items with proper CategoryID references using UPSERT
    with engine.begin() as conn:
        for _, row in items_df.iterrows():
            # Map CategoryID from the CSV to actual database CategoryID
            category_id = row['CategoryID']  # This should already be correct from our normalization
            
            conn.execute(text("""
                INSERT INTO `Items` (`ItemName`, `PricePerUnit`, `CategoryID`) 
                VALUES (:name, :price, :category_id)
                ON DUPLICATE KEY UPDATE 
                    `PricePerUnit` = VALUES(`PricePerUnit`),
                    `CategoryID` = VALUES(`CategoryID`),
                    `UpdatedAt` = CURRENT_TIMESTAMP
            """), {
                "name": row['ItemName'],
                "price": row['PricePerUnit'],
                "category_id": category_id
            })
    
    with engine.connect() as conn:
        count = conn.execute(text("SELECT COUNT(*) FROM `Items`")).scalar()
        log(f"Items loaded: {count}")
        reconcile_batch_data(engine, "Items", len(items_df), " (items migration)", is_final_check=True)

def load_transactions_table(engine):
    """Load transactions table (depends on all other tables)"""
    
    log("\nLoading Transactions table...")
    
    if not TRANSACTIONS_CSV.exists():
        log("ERROR: Transactions CSV file not found")
        return
    
    transactions_df = pd.read_csv(TRANSACTIONS_CSV)
    log(f"Loading {len(transactions_df)} transactions...")
    
    # Fix NaN values in DiscountApplied field that cause MySQL errors
    import numpy as np
    if 'DiscountApplied' in transactions_df.columns:
        # Replace any NaN values with False and ensure boolean type
        transactions_df['DiscountApplied'] = transactions_df['DiscountApplied'].fillna(False).astype(bool)
    
    # Fix NaN values in numeric fields
    numeric_fields = ['Quantity', 'TotalPrice']
    for field in numeric_fields:
        if field in transactions_df.columns:
            transactions_df[field] = transactions_df[field].fillna(0)
    
    # Load in batches for better performance
    batch_size = 1000
    total_loaded = 0
    
    with engine.begin() as conn:
        for start_idx in range(0, len(transactions_df), batch_size):
            end_idx = min(start_idx + batch_size, len(transactions_df))
            batch = transactions_df.iloc[start_idx:end_idx]
            
            for _, row in batch.iterrows():
                try:
                    # Use UPSERT (INSERT ... ON DUPLICATE KEY UPDATE) for idempotent writes
                    conn.execute(text("""
                        INSERT INTO `Transactions` 
                        (`TransactionID`, `CustomerID`, `ItemID`, `PaymentMethodID`, `LocationID`, 
                         `Quantity`, `TotalPrice`, `TransactionDate`, `DiscountApplied`) 
                        VALUES (:tid, :cid, :iid, :pmid, :lid, :qty, :total, :date, :discount)
                        ON DUPLICATE KEY UPDATE
                            `CustomerID` = VALUES(`CustomerID`),
                            `ItemID` = VALUES(`ItemID`),
                            `PaymentMethodID` = VALUES(`PaymentMethodID`),
                            `LocationID` = VALUES(`LocationID`),
                            `Quantity` = VALUES(`Quantity`),
                            `TotalPrice` = VALUES(`TotalPrice`),
                            `TransactionDate` = VALUES(`TransactionDate`),
                            `DiscountApplied` = VALUES(`DiscountApplied`)
                    """), {
                        "tid": row['TransactionID'],
                        "cid": row['CustomerID'],
                        "iid": row['ItemID'],
                        "pmid": row['PaymentMethodID'],
                        "lid": row['LocationID'],
                        "qty": row['Quantity'],
                        "total": row['TotalPrice'],
                        "date": row['TransactionDate'],
                        "discount": row['DiscountApplied']
                    })
                    total_loaded += 1
                except Exception as e:
                    log(f"Error loading transaction {row['TransactionID']}: {e}")
            
            log(f"Loaded batch {start_idx+1}-{end_idx} ({total_loaded} successful)")
            
            # Skip batch reconciliation during loading to avoid transaction isolation issues
            # Final reconciliation will be performed after all batches are complete
    
    # Final reconciliation check
    with engine.connect() as conn:
        count = conn.execute(text("SELECT COUNT(*) FROM `Transactions`")).scalar()
        log(f"Total transactions in database: {count}")
        
        # Final data integrity check
        expected_total = len(transactions_df)
        if reconcile_batch_data(engine, "Transactions", expected_total, " (final check)", is_final_check=True):
            log("âœ“ All transaction data successfully migrated and verified")
        else:
            log(f"âš  Warning: Expected {expected_total} transactions, but database has {count}")


def reconcile_batch_data(engine, table_name, expected_count, batch_info="", is_final_check=False):
    """Perform per-batch data reconciliation with improved logic"""
    
    with engine.connect() as conn:
        actual_count = conn.execute(text(f"SELECT COUNT(*) FROM `{table_name}`")).scalar()
        
        # For batch checks during processing, use >= to account for timing issues
        # For final checks, use exact matching
        if is_final_check:
            success = actual_count == expected_count
        else:
            # During batch processing, we expect the count to be at least the expected amount
            # This accounts for transaction timing and concurrent operations
            success = actual_count >= expected_count
        
        if success:
            log(f"âœ“ Batch reconciliation passed for {table_name}{batch_info}: Expected {expected_count}, Got {actual_count}")
            audit_log("RECONCILIATION_SUCCESS", f"Table {table_name}{batch_info}: Expected {expected_count}, Got {actual_count}")
            return True
        else:
            # Only log as failed if we're significantly off or it's a final check
            if is_final_check or actual_count < (expected_count * 0.9):  # Allow 10% tolerance during processing
                log(f"âœ— Batch reconciliation FAILED for {table_name}{batch_info}: Expected {expected_count}, Got {actual_count}")
                audit_log("RECONCILIATION_FAILED", f"Table {table_name}{batch_info}: Expected {expected_count}, Got {actual_count}")
            else:
                # Minor discrepancy during processing - log as warning instead of failure
                log(f"âš  Batch reconciliation minor variance for {table_name}{batch_info}: Expected {expected_count}, Got {actual_count} (within tolerance)")
                audit_log("RECONCILIATION_WARNING", f"Table {table_name}{batch_info}: Expected {expected_count}, Got {actual_count} (processing variance)")
            return False

def audit_log(event_type, message, extra_data=None):
    """Enhanced audit logging with structured format"""
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    audit_entry = f"[{timestamp}] {event_type}: {message}"
    
    if extra_data:
        audit_entry += f" | Extra: {extra_data}"
    
    # Log to console
    log(f"ðŸ” AUDIT: {audit_entry}")
    
    # Log to audit file
    audit_file = RESULTS / "migration_audit.log"
    with open(audit_file, "a", encoding="utf-8") as f:
        f.write(audit_entry + "\n")

def verify_data_integrity(engine):
    """Verify the normalized database integrity"""
    
    log("\nVerifying data integrity...")
    
    with engine.connect() as conn:
        # Check record counts
        tables = ["Categories", "Locations", "PaymentMethods", "Customers", "Items", "Transactions"]
        for table in tables:
            count = conn.execute(text(f"SELECT COUNT(*) FROM `{table}`")).scalar()
            log(f"{table}: {count:,} records")
        
        # Check foreign key integrity
        log("\nChecking foreign key integrity:")
        
        # Items -> Categories
        orphaned_items = conn.execute(text("""
            SELECT COUNT(*) FROM `Items` i 
            LEFT JOIN `Categories` c ON i.CategoryID = c.CategoryID 
            WHERE c.CategoryID IS NULL
        """)).scalar()
        log(f"Orphaned items (no category): {orphaned_items}")
        
        # Transactions -> Customers
        orphaned_customers = conn.execute(text("""
            SELECT COUNT(*) FROM `Transactions` t 
            LEFT JOIN `Customers` c ON t.CustomerID = c.CustomerID 
            WHERE c.CustomerID IS NULL
        """)).scalar()
        log(f"Transactions with invalid customers: {orphaned_customers}")
        
        # Transactions -> Items
        orphaned_trans_items = conn.execute(text("""
            SELECT COUNT(*) FROM `Transactions` t 
            LEFT JOIN `Items` i ON t.ItemID = i.ItemID 
            WHERE i.ItemID IS NULL
        """)).scalar()
        log(f"Transactions with invalid items: {orphaned_trans_items}")
        
        # Transactions -> PaymentMethods
        orphaned_payments = conn.execute(text("""
            SELECT COUNT(*) FROM `Transactions` t 
            LEFT JOIN `PaymentMethods` pm ON t.PaymentMethodID = pm.PaymentMethodID 
            WHERE pm.PaymentMethodID IS NULL
        """)).scalar()
        log(f"Transactions with invalid payment methods: {orphaned_payments}")
        
        # Transactions -> Locations
        orphaned_locations = conn.execute(text("""
            SELECT COUNT(*) FROM `Transactions` t 
            LEFT JOIN `Locations` l ON t.LocationID = l.LocationID 
            WHERE l.LocationID IS NULL
        """)).scalar()
        log(f"Transactions with invalid locations: {orphaned_locations}")
        
        # Business rule checks
        log("\nBusiness rule validation:")
        
        # Check for negative quantities
        negative_qty = conn.execute(text("""
            SELECT COUNT(*) FROM `Transactions` WHERE Quantity <= 0
        """)).scalar()
        log(f"Transactions with negative/zero quantity: {negative_qty}")
        
        # Check for negative prices
        negative_price = conn.execute(text("""
            SELECT COUNT(*) FROM `Transactions` WHERE TotalPrice <= 0
        """)).scalar()
        log(f"Transactions with negative/zero total price: {negative_price}")

def main():
    """Main execution function with enhanced audit logging"""
    
    # Clear previous log
    if LOG.exists():
        LOG.unlink()
    
    log("Starting normalized MySQL migration...")
    log(f"Target database: {MYSQL_DB}")
    audit_log("MIGRATION_START", f"Starting normalized MySQL migration to database: {MYSQL_DB}")
    
    try:
        # Check if all required CSV files exist
        required_files = [CATEGORIES_CSV, LOCATIONS_CSV, PAYMENT_METHODS_CSV, 
                         CUSTOMERS_CSV, ITEMS_CSV, TRANSACTIONS_CSV]
        
        missing_files = [f for f in required_files if not f.exists()]
        if missing_files:
            log("ERROR: Missing required normalized CSV files:")
            for f in missing_files:
                log(f"  {f}")
            log("Please run 01_data_pipeline_main.py first to generate normalized CSV files")
            audit_log("MIGRATION_ERROR", f"Missing required CSV files: {missing_files}")
            return
        
        audit_log("FILE_VALIDATION", f"All required CSV files found: {len(required_files)} files")
        
        # Create database
        create_database()
        audit_log("DATABASE_CREATED", f"Database {MYSQL_DB} created/verified")
        
        # Connect to the target database
        engine = create_engine(mysql_url(MYSQL_DB), pool_pre_ping=True)
        audit_log("CONNECTION_ESTABLISHED", f"Database connection established to {MYSQL_DB}")
        
        # Create normalized schema
        create_normalized_schema(engine)
        audit_log("SCHEMA_CREATED", "Normalized schema tables created/verified")
        
        # Load data in correct order (respecting foreign key dependencies)
        load_reference_tables(engine)
        audit_log("REFERENCE_TABLES_LOADED", "All reference tables (Categories, Locations, PaymentMethods, Customers) loaded")
        
        load_items_table(engine)
        audit_log("ITEMS_TABLE_LOADED", "Items table with foreign key relationships loaded")
        
        load_transactions_table(engine)
        audit_log("TRANSACTIONS_TABLE_LOADED", "Transactions table with all foreign key relationships loaded")
        
        # Verify data integrity
        verify_data_integrity(engine)
        audit_log("DATA_INTEGRITY_VERIFIED", "All data integrity checks completed")
        
        log("\n" + "=" * 60)
        log("NORMALIZED MYSQL MIGRATION COMPLETE!")
        log("=" * 60)
        log(f"Database: {MYSQL_DB}")
        log(f"Schema saved to: {DDL}")
        log(f"Full log saved to: {LOG}")
        audit_log("MIGRATION_SUCCESS", f"Complete migration finished successfully. Database: {MYSQL_DB}, Log: {LOG}")
        
    except Exception as e:
        log(f"ERROR during migration: {str(e)}")
        audit_log("MIGRATION_ERROR", f"Migration failed with error: {str(e)}")
        raise

if __name__ == "__main__":
    main()
